---
title: "L03 Data Imports, Joins, Tidy Data"
subtitle: "Foundations of Data Science with R (STAT 359)"
author: "Stefanie Huttelmaier"

format:
  html:
    toc: true
    embed-resources: true
    code-fold: show
    link-external-newwindow: true

execute:
  warning: false
  
from: markdown+emoji 
---

::: {.callout-tip icon=false}

## Github Repo Link

[https://github.com/STAT359-2024SU/l03-import-join-tidy-sahutt.git](https://github.com/STAT359-2024SU/l03-import-join-tidy-sahutt.git)

:::

## Overview

The goal of this lab is to briefly cover a variety of topics to allow you to import and work with messy data. 

- Utilize the `readr` package, a member of the `tidyverse`, to load flat files (e.g., `csv` & `txt`) and to gain a better understanding of the basic data structure called a "tibble". Tibbles are data frames, but they tweak some of R's older behaviors to make life a little easier by avoiding unintentional mistakes/errors. The `tibble` package, another member of the `tidyverse`, provides the framework for these opinionated data frames which make working with data in the tidyverse possible.

- Understand the concepts of **relational data**. It is extremely rare that data analyses involve one all-encompassing dataset; we usually want to combine information from multiple data tables/sources to answer interesting questions. The collection of data tables/sources is called **relational data** because it is the relations connecting the datasets together that are important.

- Learn what it means to be a "tidy" dataset and how to tidy messy datasets utilizing the `tidyr` package -- a core member of the `tidyverse`. 

Useful resources:

- [`tibble` package home page](http://tibble.tidyverse.org/index.html).
- [`readr` package home page](http://readr.tidyverse.org/articles/readr.html)
- See [two-table verbs in `dplyr`](https://dplyr.tidyverse.org/articles/two-table.html) for more information concerning relational data. 
- [Relational Database Wikipedia Page](https://en.wikipedia.org/wiki/Relational_database)
- [`tidyr` package home page](http://tidyr.tidyverse.org/)
- [pivoting vignette](https://tidyr.tidyverse.org/articles/pivot.html)

**Additionally, you will learn how to ignore files that are too large for version control!**

## Load packages

You should always begin by loading all necessary packages towards the beginning of your document.

```{r}
#| label: load-pkgs
#| code-fold: false

# Loading package(s)

library(tidyverse)
library(nycflights13)

```


## Datasets 

All datasets are either coded inline, contained in the `data` sub-directory, or found within R packages, which students should be able to identify and download as needed.

The Case Study dataset is stored in the `data` folder and called `users_top7_2020.csv`.

```{r}
#| label: load-data
#| code-fold: false

# Load dataset(s)

```

## Exercises: Data Import

### Exercise 1

Demonstrate how to read in `TopBabyNamesByState.txt` contained in the `data` sub-directory using the appropriate function from the `readr` package. After reading in the data, determine the top male and female names in 1984 for South Dakota.

::: {.callout-tip icon="false"}
## Solution

```{r}

top_baby_names <- read_delim("data/TopBabyNamesbyState.txt",
                             show_col_types = FALSE)

top_baby_names |>
  filter(state == 'SD', year == 1984) |> 
  knitr::kable()

```

:::

### Exercise 2

What is the difference between `read.csv()` and `read_csv()`? Which is the appropriate `tidyverse` function to use for importing a comma delimited file?

::: {.callout-tip icon="false"}
## Solution

- read.csv() is base R - reads data in as a data frame  
- read_csv() is from tidyverse - reads data in as a tibble  
Both functions read in a comma delimited file, but tidyverse will always use an underscore in the function 

A tibble is easier and more consistent to work with, prints to screen in an abbreviated manner, and always outputs a tibble.

:::

### Exercise 3

Practice referring to non-syntactic names in the following data frame by:

```{r}
#| label: ex-03

# toy dataset
annoying <- tibble(
  `1` = 1:10,
  `2` = `1` * 2 + rnorm(length(`1`))
)
```

a.  Extracting the variable called 1.
b.  Plotting a scatterplot of 1 vs 2.
c.  Creating a new column called 3 which is 2 divided by 1.
d.  Renaming the columns to one, two and three.

::: {.callout-tip icon="false"}
## Solution

```{r}
#a. 
annoying[ , 1]

#b.
annoying |> 
  ggplot(aes(x = `2`, y = `1`)) +
  geom_point()

#c.
annoying |> 
  mutate('3' = `2`/`1`)

#d. 
annoying |> 
  mutate('3' = `2`/`1`) |>  
  rename( one = `1`, two = `2`, three = '3')

```


:::

### Exercise 4

What function in `janitor` helps you deal with non-syntactic column names in R and and also ensures column names are systematically handled? Demonstrate its use on the `annoying` dataset above.

::: {.callout-tip icon="false"}
## Solution

The janitor function that systematically fixes column names is clean_names()
```{r}

janitor::clean_names(annoying)

```


:::

### Exercise 5

Demonstrate how to manually input the data table below into R using each of these functions:

-   `tibble()`
-   `tribble()`

| price | store   | ounces |
|-------|---------|--------|
| 3.99  | target  | 128    |
| 3.75  | walmart | 128    |
| 3.00  | amazon  | 128    |

::: {.callout-tip icon="false"}
## Solution

```{r}

tibble(
  price = c(3.99, 3.75, 3.00),
  store = c("target", "walmart", "amazon"),
  ounces = c(128, 128, 128))

tribble(
  ~price, ~store, ~ounces,
  3.99, "target", 128,
  3.75, "walmart", 128,
  3.00, "amazon", 128)

```

:::



### Exercise 6

::: {.callout-important}
GitHub (free account) cannot store large files. If you try to commit and push a large dataset you will have an ERROR! Any file over 100 MB (100,000 KB) needs to be added to the `.gitignore` file BEFORE committing.

**We need to do that for this exercise!**
:::

1. Start by committing and pushing your current work to GitHub! 
1. Then download the `cc-est2016-alldata.csv` file from Canvas and add it to the `data` subdirectory. **Do not commit!** We need to add the file to the `.gitignore` file first.
1. **Add `cc-est2016-alldata.csv` to the .gitignore** file. That is, add `data/cc-est2016-alldata.csv` to the file with an appropriate header. If the file has been added (meaning ignored) correctly, it will NOT appear in the Git pane to commit --- may need to refresh the pane. 
1. Once the file is successfully ignored, commit with the comment "large data ignored!"

Now that you have taken care of the large file issue, read the file in and just print the first 5 observations.

::: {.callout-note collapse=true}

## Oh no, I commited a large file!

If you Commit a large file and try to push to GitHub you will have an issue! Do NOT keep clicking Commit. You **MUST UNDO** the Commit issue before moving forward. The more times you click Commit and generate the error the more Commits you will then need to undo. To undo a Commit, in the Terminal type: `git reset --soft HEAD~1`

To automatically find and add files over 100MB to the .gitignore you can type the following code in the Terminal: 

```{bash}
#| label: terminal
#| code-fold: false
#| eval: false

find . -size +100M | sed 's|^\./||g' | cat >> .gitignore; awk '!NF || !seen[$0]++' .gitignore
```

Note: You will need to retype this **EVERY** time a new file over 100MB is added to your project.
:::

::: {.callout-tip icon="false"}
## Solution

```{r}

population_data <- read_csv("data/cc-est2016-alldata.csv", 
         show_col_types = FALSE)

head(population_data, n = 5)

```


:::

## Exercises: Joining Data

### Exercise 7 

Describe in your own words what the functions `left_join`, `right_join`, `inner_join`, and `full_join` accomplish.

::: {.callout-tip icon="false"}
## Solution

- left_join() keeps all the rows in the 'left' table, or x. Values from the y table are joined to the x table from the right. Rows that exist in x but not y are given an NA value. 

- right_join() keeps all the rows from the 'right' table, or the y table. Rows that exist in y but not in x are given an NA value. 

- inner_join() only keeps rows that occur in both x and y tables, no NA values are created

-full_join() keeps everything from x and y, NA values are created for rows that exist exclusively in x or y 

:::


### Exercise 8

What weather conditions make it more likely to see a departure delay? 

Hint: Join the two appropriate datasets together, then determine how weather impacts departures.

::: {.callout-tip icon="false"}
## Solution

I was expecting wind speed or precipitation to have some impact on departure delay but the most common weather during delays is low visibility. Fog causes delays because it interferes with instrumentation and pilots cannot navigate by sight.

```{r}

flights_weather <- weather |> 
  left_join(flights)  
  #by = join_by(origin, year, month, day, hour, time_hour)
  
flights_weather |> select(temp:visib, dep_delay)  |> 
  arrange(desc(dep_delay))

```

:::

### Exercise 9

Consider the data sets created below called `data_2022` and `data_2023`. 
Use `bind_rows()` to combine `data_2022` and `data_2023`.  

When might you use `bind_rows` or `bind_cols` instead of a `join`?

```{r}
#| label: ex-09
#| eval: false

# dataset/table
data_2022 <- tribble(
  ~year,  ~id,      ~income, ~clients
  2022,   "x01932",  "$45000",   10,
  2022,   "x32912",  "$60000",   15,
  2022,   "x80188",  "$37000",   8
)

data_2023 <- tribble(
  ~Year,  ~Id,      ~Income, ~Clients
  2023,   "x79320",  "$51000",   9,
  2023,   "x42215",  "$48000",   11,
  2023,   "x32912",  "$75000",   20
)
```


::: {.callout-tip icon="false"}
## Solution

You would use bind row when your observations are different. You are essentially adding more unique observations to your data frame. Columns will bind by name and missing columns will be filled in with an NA. Basically join, but rows instead of columns. 

You should probably never opt for bind_cols over join. If you know all your observations are the same and ordered correctly, it will bind columns together without a key which can make nonsense results. 

:::

## Exercises: Tidying Data

### Exercise 10

Tidy the simple tibble of [M&M](https://en.wikipedia.org/wiki/M%26M%27s) data below and drop the NA values.

Do you need to make it wider or longer? What are the variables in your tidy version? What argument drops the NA values?

```{r}
#| label: ex-10


# simple tibble of M&M data
mm_data <- tribble(
  ~mm_type, ~blue, ~orange,	~green,	~yellow, ~brown, ~red, ~cyan_blue,
  "plain",  6,     18,      12,	    6,       7,	     7,    NA,
  "peanut", NA,	   11,	    9,	    1,	     12,	   8,    15
)

```

::: {.callout-tip icon="false"}
## Solution


```{r}

mmtidy <- mm_data |> 
  pivot_longer(cols = !mm_type, 
               names_to = "color",
               values_to = "count",
               values_drop_na = TRUE) |> 
  knitr::kable()

mmtidy

```


:::

### Exercise 11

Use `table4a` and **only** the `pivot_longer` function to recreate the following (see instructions):


::: {.callout-tip icon="false"}
## Solution


```{r}
tidy_table4a <- table4a |> 
  pivot_longer(cols = !country,
               names_to = "year",
               values_to = "cases")

knitr::kable(tidy_table4a)
```


:::


### Exercise 12

What happens if you use `pivot_wider()` on this table so that we have a dataset with 3 columns (`respondent_name`, `age`, `height`) and why? 

Fix the issue by adding a new column.

```{r}
#| label: ex-12
#| eval: false

# dataset/table
people <- tribble(
  ~respondent_name,  ~key,    ~value,
  #-----------------|--------|------
  "Phillip Woods",   "age",       45,
  "Phillip Woods",   "height",   186,
  "Phillip Woods",   "age",       50,
  "Jessica Cordero", "age",       37,
  "Jessica Cordero", "height",   156
)
```

::: {.callout-tip icon="false"}
## Solution

The respondent Phillip Woods either erroneously has two ages in the key, or there are two patients named Phillip Woods. Since there is no unique key, pivot wider is unable to pivot two values into one cell. 

```{r}

people2 <- tribble(
  ~respondent_name,  ~key,    ~value, ~ID,
  #-----------------|--------|------|-----|
  "Phillip Woods",   "age",       45, 1,
  "Phillip Woods",   "height",   186, 1,
  "Phillip Woods",   "age",       50, 2,
  "Jessica Cordero", "age",       37, 3,
  "Jessica Cordero", "height",   156, 3
)

people2 |> pivot_wider(names_from = key,
                      values_from = value) |> 
  knitr::kable()
```


:::

## Case Study

Tinder is interested in knowing more about their most active users and have have tasked you with exploring their 7 most active users during 2020. The dataset containing the top 7 active users during 2020 is stored in the `data` folder and called `users_top7_2020.csv`.^[This dataset was sourced from [Swipestats.io](https://www.swipestats.io/).]

The column names contain prefixes "matches", "likes", and "passes" followed by a number; the suffix number represents the month; and the cell represents either the total number of matches (matches), total number of times the user swiped right (likes), or total number of times the user swiped left (passes) during that month (ie: `matches_1` = total number of matches during January 2020).

Complete the following tasks:

a)  Use an appropriate readr function to read in `users_top7_2020.csv`

b)   Is `users_top7_2020.csv` a `tibble` and how do you know? If it isn't, then figure how to turn it into a `tibble`.

c) Use an appropriate graph to visualize the matches, likes, and passes over time for each user. What insights and conclusions can you gain from this graph, if any.

d)   Write out a copy of the tidy dataset to the `data` sub-directory as an RDS file named `users_tidy.rds`. 

e) What is one benefit of saving a dataset as an `rds` compared to a `csv`?

::: {.callout-tip icon="false"}
## Solution

We use read_csv() to read in `users_top7_2020.csv` because it is a comma separated variable document, and we know it is a tibble since we're using tidyr to read it in. However, we can confirm it is a tibble using is_tibble(). If it was not, we could us the as_tibble() function to convert it. 

Likes and passes tend to follow the same pattern, showing periods of activity for each user. These patterns tend to ebb and flow. Four of the top users used the app throughout 2020, while three of the top users used the app mostly in the first half of the year. Matches tended to remain low, even with large spikes in likes and passes, regardless of the time of year.

Saving as an rds file saves our document as an r object. When we reimport it, the changes we made to month as a dbl data type will remain. 
```{r}
#| echo: false

top_user_2020 <- read_csv("data/users_top7_2020.csv")

users_tidy <- top_user_2020 |> 
  pivot_longer(cols = !user_id,
               names_to = c("category", "month"),
               names_sep = "_",
               values_to = "count")  |> 
  mutate(month = as.numeric(month))

users_tidy |> 
  ggplot(aes(x = month, y = count, color = category)) +
  geom_line() +
  facet_wrap(~user_id, scales = "free_y") +
  scale_x_continuous(name ="Month", 
                  breaks=c(2, 4, 6, 8, 10, 12)) +
  labs(y = "Count",
       title = "Top 7 user behavior by month in 2020")

```


:::


